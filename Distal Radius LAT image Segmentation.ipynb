{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import *\n",
    "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D, concatenate, Activation,Conv2DTranspose\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, History, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import backend as keras\n",
    "from keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import cv2\n",
    "from segmentation_GP import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkfolder(folder):\n",
    "\n",
    "    for j in range(len(folder)):\n",
    "        if not os.path.lexists(folder[j]):\n",
    "            os.makedirs(folder[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data ##\n",
    "data_path = './3_deepdata/1_exp1/train/LAT_aug_img/'\n",
    "label_path = './3_deepdata/1_exp1/train/LAT_aug_label/'\n",
    "\n",
    "## test data ##\n",
    "test_path = './3_deepdata/1_exp1/test/LAT_pre_img/'\n",
    "test_label_path = './3_deepdata/1_exp1/test/LAT_pre_label/'\n",
    "\n",
    "## npy file ##\n",
    "npy_path = './3_deepdata/1_exp1/npy/LAT/'\n",
    "check_model_path = './4_result/exp1/snapshot/LAT/'\n",
    "predict_path = './4_result/exp1/result/LAT/'\n",
    "\n",
    "## img ##\n",
    "img_rows, img_cols = 512, 512\n",
    "img_type = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = [npy_path, check_model_path, predict_path]\n",
    "mkfolder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_train_data(data_path, label_path, npy_path, img_rows, img_cols, 'train', img_type)\n",
    "# create_test_data(test_path, test_label_path, npy_path, img_rows, img_cols, 'test', img_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "load train images...\n",
      "------------------------------\n",
      "img :  255.0\n",
      "mask :  255.0\n",
      "------------------------------\n",
      "normalization start...\n",
      "------------------------------\n",
      "img :  1.0\n",
      "mask :  1.0\n",
      "------------------------------\n",
      "load test images...\n",
      "------------------------------\n",
      "(10130, 512, 512, 1)\n",
      "(10130, 512, 512, 1)\n",
      "(254, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "imgs_train, imgs_mask_train, imgs_test = load_data(npy_path+'train.npy', \n",
    "                                                   npy_path+'train_label.npy', \n",
    "                                                   npy_path+'test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(img_rows, img_cols):\n",
    "    inputs = Input((img_rows, img_cols,1))\n",
    "    conv1 = Conv2D(32, (3, 3), activation=None, padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation=None, padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation=None, padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation=None, padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation=None, padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation=None, padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation=None, padding='same')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation=None, padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation=None, padding='same')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation=None, padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=None, padding='same')(up6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=None, padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=None, padding='same')(up7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=None, padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=None, padding='same')(up8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=None, padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=None, padding='same')(up9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=None, padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "def sens(y_target, y_pred): # sensitivity, recall\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sch(epoch):\n",
    "    if epoch>100 and epoch<=250:\n",
    "        return 0.0001\n",
    "    elif epoch>250:\n",
    "        return 0.00001\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "load unet model\n",
      "------------------------------\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print(\"load unet model\")\n",
    "print('-'*30)\n",
    "\n",
    "model = get_unet(img_rows, img_cols)\n",
    "model = multi_gpu_model(model,gpus=2)\n",
    "model.compile(optimizer=Adam(lr=1e-3), loss=dice_coef_loss, metrics=['accuracy', sens, dice_coef_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(check_model_path+'lat_aug_exp1_{epoch:d}_{loss:f}.h5', monitor='val_dice_coef_loss',verbose=1, save_best_only=True)\n",
    "# sc = LearningRateScheduler(sch)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_dice_coef_loss', factor=0.8, min_delta = 0.01, patience=5, min_lr=1e-6, verbose=1)\n",
    "earlystopping = EarlyStopping(monitor='val_dice_coef_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Train on 8104 samples, validate on 2026 samples\n",
      "Epoch 1/200\n",
      "8104/8104 [==============================] - 261s 32ms/step - loss: 0.2174 - accuracy: 0.9748 - sens: 0.8947 - dice_coef_loss: 0.2173 - val_loss: 0.1541 - val_accuracy: 0.9840 - val_sens: 0.8669 - val_dice_coef_loss: 0.1539\n",
      "\n",
      "Epoch 00001: val_dice_coef_loss improved from inf to 0.15392, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_1_0.217409.h5\n",
      "Epoch 2/200\n",
      "8104/8104 [==============================] - 247s 30ms/step - loss: 0.0833 - accuracy: 0.9913 - sens: 0.9293 - dice_coef_loss: 0.0833 - val_loss: 0.1248 - val_accuracy: 0.9865 - val_sens: 0.9018 - val_dice_coef_loss: 0.1247\n",
      "\n",
      "Epoch 00002: val_dice_coef_loss improved from 0.15392 to 0.12474, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_2_0.083321.h5\n",
      "Epoch 3/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0651 - accuracy: 0.9931 - sens: 0.9450 - dice_coef_loss: 0.0651 - val_loss: 0.0772 - val_accuracy: 0.9916 - val_sens: 0.9533 - val_dice_coef_loss: 0.0772\n",
      "\n",
      "Epoch 00003: val_dice_coef_loss improved from 0.12474 to 0.07722, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_3_0.065087.h5\n",
      "Epoch 4/200\n",
      "8104/8104 [==============================] - 249s 31ms/step - loss: 0.0553 - accuracy: 0.9941 - sens: 0.9528 - dice_coef_loss: 0.0553 - val_loss: 0.0697 - val_accuracy: 0.9926 - val_sens: 0.9468 - val_dice_coef_loss: 0.0696\n",
      "\n",
      "Epoch 00004: val_dice_coef_loss improved from 0.07722 to 0.06962, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_4_0.055344.h5\n",
      "Epoch 5/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0502 - accuracy: 0.9946 - sens: 0.9562 - dice_coef_loss: 0.0502 - val_loss: 0.0902 - val_accuracy: 0.9908 - val_sens: 0.9029 - val_dice_coef_loss: 0.0904\n",
      "\n",
      "Epoch 00005: val_dice_coef_loss did not improve from 0.06962\n",
      "Epoch 6/200\n",
      "8104/8104 [==============================] - 247s 31ms/step - loss: 0.0454 - accuracy: 0.9951 - sens: 0.9605 - dice_coef_loss: 0.0454 - val_loss: 0.0707 - val_accuracy: 0.9923 - val_sens: 0.9682 - val_dice_coef_loss: 0.0707\n",
      "\n",
      "Epoch 00006: val_dice_coef_loss did not improve from 0.06962\n",
      "Epoch 7/200\n",
      "8104/8104 [==============================] - 247s 30ms/step - loss: 0.0415 - accuracy: 0.9955 - sens: 0.9637 - dice_coef_loss: 0.0415 - val_loss: 0.0562 - val_accuracy: 0.9940 - val_sens: 0.9605 - val_dice_coef_loss: 0.0562\n",
      "\n",
      "Epoch 00007: val_dice_coef_loss improved from 0.06962 to 0.05618, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_7_0.041521.h5\n",
      "Epoch 8/200\n",
      "8104/8104 [==============================] - 247s 31ms/step - loss: 0.0402 - accuracy: 0.9957 - sens: 0.9641 - dice_coef_loss: 0.0402 - val_loss: 0.0707 - val_accuracy: 0.9926 - val_sens: 0.9309 - val_dice_coef_loss: 0.0706\n",
      "\n",
      "Epoch 00008: val_dice_coef_loss did not improve from 0.05618\n",
      "Epoch 9/200\n",
      "8104/8104 [==============================] - 247s 30ms/step - loss: 0.0376 - accuracy: 0.9960 - sens: 0.9675 - dice_coef_loss: 0.0376 - val_loss: 0.0553 - val_accuracy: 0.9942 - val_sens: 0.9437 - val_dice_coef_loss: 0.0553\n",
      "\n",
      "Epoch 00009: val_dice_coef_loss improved from 0.05618 to 0.05526, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_9_0.037606.h5\n",
      "Epoch 10/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0358 - accuracy: 0.9962 - sens: 0.9684 - dice_coef_loss: 0.0358 - val_loss: 0.0549 - val_accuracy: 0.9943 - val_sens: 0.9370 - val_dice_coef_loss: 0.0549\n",
      "\n",
      "Epoch 00010: val_dice_coef_loss improved from 0.05526 to 0.05488, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_10_0.035763.h5\n",
      "Epoch 11/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0341 - accuracy: 0.9963 - sens: 0.9701 - dice_coef_loss: 0.0341 - val_loss: 0.0553 - val_accuracy: 0.9942 - val_sens: 0.9459 - val_dice_coef_loss: 0.0553\n",
      "\n",
      "Epoch 00011: val_dice_coef_loss did not improve from 0.05488\n",
      "Epoch 12/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0330 - accuracy: 0.9965 - sens: 0.9710 - dice_coef_loss: 0.0330 - val_loss: 0.0570 - val_accuracy: 0.9941 - val_sens: 0.9292 - val_dice_coef_loss: 0.0570\n",
      "\n",
      "Epoch 00012: val_dice_coef_loss did not improve from 0.05488\n",
      "Epoch 13/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0313 - accuracy: 0.9966 - sens: 0.9723 - dice_coef_loss: 0.0313 - val_loss: 0.4956 - val_accuracy: 0.9653 - val_sens: 0.3547 - val_dice_coef_loss: 0.4948\n",
      "\n",
      "Epoch 00013: val_dice_coef_loss did not improve from 0.05488\n",
      "Epoch 14/200\n",
      "8104/8104 [==============================] - 247s 30ms/step - loss: 0.0314 - accuracy: 0.9966 - sens: 0.9719 - dice_coef_loss: 0.0315 - val_loss: 0.0607 - val_accuracy: 0.9937 - val_sens: 0.9412 - val_dice_coef_loss: 0.0606\n",
      "\n",
      "Epoch 00014: val_dice_coef_loss did not improve from 0.05488\n",
      "Epoch 15/200\n",
      "8104/8104 [==============================] - 245s 30ms/step - loss: 0.0304 - accuracy: 0.9967 - sens: 0.9732 - dice_coef_loss: 0.0304 - val_loss: 0.0488 - val_accuracy: 0.9948 - val_sens: 0.9647 - val_dice_coef_loss: 0.0487\n",
      "\n",
      "Epoch 00015: val_dice_coef_loss improved from 0.05488 to 0.04874, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_15_0.030420.h5\n",
      "Epoch 16/200\n",
      "8104/8104 [==============================] - 245s 30ms/step - loss: 0.0282 - accuracy: 0.9970 - sens: 0.9751 - dice_coef_loss: 0.0282 - val_loss: 0.0458 - val_accuracy: 0.9952 - val_sens: 0.9556 - val_dice_coef_loss: 0.0458\n",
      "\n",
      "Epoch 00016: val_dice_coef_loss improved from 0.04874 to 0.04581, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_16_0.028161.h5\n",
      "Epoch 17/200\n",
      "8104/8104 [==============================] - 247s 30ms/step - loss: 0.0281 - accuracy: 0.9970 - sens: 0.9752 - dice_coef_loss: 0.0281 - val_loss: 0.0541 - val_accuracy: 0.9941 - val_sens: 0.9631 - val_dice_coef_loss: 0.0541\n",
      "\n",
      "Epoch 00017: val_dice_coef_loss did not improve from 0.04581\n",
      "Epoch 18/200\n",
      "8104/8104 [==============================] - 245s 30ms/step - loss: 0.0271 - accuracy: 0.9971 - sens: 0.9762 - dice_coef_loss: 0.0271 - val_loss: 0.0431 - val_accuracy: 0.9954 - val_sens: 0.9640 - val_dice_coef_loss: 0.0431\n",
      "\n",
      "Epoch 00018: val_dice_coef_loss improved from 0.04581 to 0.04307, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_18_0.027060.h5\n",
      "Epoch 19/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0280 - accuracy: 0.9970 - sens: 0.9752 - dice_coef_loss: 0.0280 - val_loss: 0.0533 - val_accuracy: 0.9944 - val_sens: 0.9530 - val_dice_coef_loss: 0.0532\n",
      "\n",
      "Epoch 00019: val_dice_coef_loss did not improve from 0.04307\n",
      "Epoch 20/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0284 - accuracy: 0.9970 - sens: 0.9748 - dice_coef_loss: 0.0284 - val_loss: 0.0683 - val_accuracy: 0.9930 - val_sens: 0.9203 - val_dice_coef_loss: 0.0682\n",
      "\n",
      "Epoch 00020: val_dice_coef_loss did not improve from 0.04307\n",
      "Epoch 21/200\n",
      "8104/8104 [==============================] - 247s 30ms/step - loss: 0.0244 - accuracy: 0.9974 - sens: 0.9786 - dice_coef_loss: 0.0244 - val_loss: 0.0418 - val_accuracy: 0.9956 - val_sens: 0.9666 - val_dice_coef_loss: 0.0417\n",
      "\n",
      "Epoch 00021: val_dice_coef_loss improved from 0.04307 to 0.04175, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_21_0.024416.h5\n",
      "Epoch 22/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0239 - accuracy: 0.9974 - sens: 0.9789 - dice_coef_loss: 0.0239 - val_loss: 0.0451 - val_accuracy: 0.9954 - val_sens: 0.9465 - val_dice_coef_loss: 0.0450\n",
      "\n",
      "Epoch 00022: val_dice_coef_loss did not improve from 0.04175\n",
      "Epoch 23/200\n",
      "8104/8104 [==============================] - 247s 31ms/step - loss: 0.0248 - accuracy: 0.9973 - sens: 0.9782 - dice_coef_loss: 0.0248 - val_loss: 0.0396 - val_accuracy: 0.9959 - val_sens: 0.9647 - val_dice_coef_loss: 0.0396\n",
      "\n",
      "Epoch 00023: val_dice_coef_loss improved from 0.04175 to 0.03957, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_23_0.024827.h5\n",
      "Epoch 24/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0231 - accuracy: 0.9975 - sens: 0.9798 - dice_coef_loss: 0.0231 - val_loss: 0.0396 - val_accuracy: 0.9959 - val_sens: 0.9648 - val_dice_coef_loss: 0.0395\n",
      "\n",
      "Epoch 00024: val_dice_coef_loss improved from 0.03957 to 0.03955, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_24_0.023073.h5\n",
      "Epoch 25/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0230 - accuracy: 0.9975 - sens: 0.9796 - dice_coef_loss: 0.0230 - val_loss: 0.0462 - val_accuracy: 0.9952 - val_sens: 0.9489 - val_dice_coef_loss: 0.0462\n",
      "\n",
      "Epoch 00025: val_dice_coef_loss did not improve from 0.03955\n",
      "Epoch 26/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0224 - accuracy: 0.9976 - sens: 0.9801 - dice_coef_loss: 0.0224 - val_loss: 0.0667 - val_accuracy: 0.9933 - val_sens: 0.9159 - val_dice_coef_loss: 0.0666\n",
      "\n",
      "Epoch 00026: val_dice_coef_loss did not improve from 0.03955\n",
      "Epoch 27/200\n",
      "8104/8104 [==============================] - 250s 31ms/step - loss: 0.0224 - accuracy: 0.9976 - sens: 0.9802 - dice_coef_loss: 0.0224 - val_loss: 0.0916 - val_accuracy: 0.9899 - val_sens: 0.9579 - val_dice_coef_loss: 0.0916\n",
      "\n",
      "Epoch 00027: val_dice_coef_loss did not improve from 0.03955\n",
      "Epoch 28/200\n",
      "8104/8104 [==============================] - 249s 31ms/step - loss: 0.0224 - accuracy: 0.9976 - sens: 0.9802 - dice_coef_loss: 0.0224 - val_loss: 0.0427 - val_accuracy: 0.9955 - val_sens: 0.9649 - val_dice_coef_loss: 0.0427\n",
      "\n",
      "Epoch 00028: val_dice_coef_loss did not improve from 0.03955\n",
      "Epoch 29/200\n",
      "8104/8104 [==============================] - 247s 30ms/step - loss: 0.0209 - accuracy: 0.9978 - sens: 0.9816 - dice_coef_loss: 0.0209 - val_loss: 0.0522 - val_accuracy: 0.9946 - val_sens: 0.9414 - val_dice_coef_loss: 0.0521\n",
      "\n",
      "Epoch 00029: val_dice_coef_loss did not improve from 0.03955\n",
      "Epoch 30/200\n",
      "8104/8104 [==============================] - 249s 31ms/step - loss: 0.0211 - accuracy: 0.9977 - sens: 0.9811 - dice_coef_loss: 0.0211 - val_loss: 0.0461 - val_accuracy: 0.9951 - val_sens: 0.9673 - val_dice_coef_loss: 0.0460\n",
      "\n",
      "Epoch 00030: val_dice_coef_loss did not improve from 0.03955\n",
      "Epoch 31/200\n",
      "8104/8104 [==============================] - 247s 30ms/step - loss: 0.0211 - accuracy: 0.9977 - sens: 0.9812 - dice_coef_loss: 0.0211 - val_loss: 0.0427 - val_accuracy: 0.9956 - val_sens: 0.9558 - val_dice_coef_loss: 0.0426\n",
      "\n",
      "Epoch 00031: val_dice_coef_loss did not improve from 0.03955\n",
      "Epoch 32/200\n",
      "8104/8104 [==============================] - 245s 30ms/step - loss: 0.0213 - accuracy: 0.9977 - sens: 0.9812 - dice_coef_loss: 0.0213 - val_loss: 0.0397 - val_accuracy: 0.9958 - val_sens: 0.9658 - val_dice_coef_loss: 0.0397\n",
      "\n",
      "Epoch 00032: val_dice_coef_loss did not improve from 0.03955\n",
      "Epoch 33/200\n",
      "8104/8104 [==============================] - 245s 30ms/step - loss: 0.0193 - accuracy: 0.9979 - sens: 0.9828 - dice_coef_loss: 0.0193 - val_loss: 0.0387 - val_accuracy: 0.9959 - val_sens: 0.9690 - val_dice_coef_loss: 0.0386\n",
      "\n",
      "Epoch 00033: val_dice_coef_loss improved from 0.03955 to 0.03864, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_33_0.019320.h5\n",
      "Epoch 34/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0188 - accuracy: 0.9980 - sens: 0.9832 - dice_coef_loss: 0.0188 - val_loss: 0.0400 - val_accuracy: 0.9959 - val_sens: 0.9606 - val_dice_coef_loss: 0.0399\n",
      "\n",
      "Epoch 00034: val_dice_coef_loss did not improve from 0.03864\n",
      "Epoch 35/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0193 - accuracy: 0.9979 - sens: 0.9828 - dice_coef_loss: 0.0193 - val_loss: 0.0399 - val_accuracy: 0.9958 - val_sens: 0.9632 - val_dice_coef_loss: 0.0399\n",
      "\n",
      "Epoch 00035: val_dice_coef_loss did not improve from 0.03864\n",
      "Epoch 36/200\n",
      "8104/8104 [==============================] - 249s 31ms/step - loss: 0.0212 - accuracy: 0.9977 - sens: 0.9808 - dice_coef_loss: 0.0212 - val_loss: 0.0659 - val_accuracy: 0.9932 - val_sens: 0.9216 - val_dice_coef_loss: 0.0658\n",
      "\n",
      "Epoch 00036: val_dice_coef_loss did not improve from 0.03864\n",
      "Epoch 37/200\n",
      "8104/8104 [==============================] - 250s 31ms/step - loss: 0.0200 - accuracy: 0.9979 - sens: 0.9821 - dice_coef_loss: 0.0200 - val_loss: 0.0380 - val_accuracy: 0.9960 - val_sens: 0.9657 - val_dice_coef_loss: 0.0380\n",
      "\n",
      "Epoch 00037: val_dice_coef_loss improved from 0.03864 to 0.03797, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_37_0.019956.h5\n",
      "Epoch 38/200\n",
      "8104/8104 [==============================] - 245s 30ms/step - loss: 0.0181 - accuracy: 0.9981 - sens: 0.9839 - dice_coef_loss: 0.0181 - val_loss: 0.0390 - val_accuracy: 0.9959 - val_sens: 0.9662 - val_dice_coef_loss: 0.0389\n",
      "\n",
      "Epoch 00038: val_dice_coef_loss did not improve from 0.03797\n",
      "Epoch 39/200\n",
      "8104/8104 [==============================] - 245s 30ms/step - loss: 0.0176 - accuracy: 0.9981 - sens: 0.9842 - dice_coef_loss: 0.0176 - val_loss: 0.0401 - val_accuracy: 0.9958 - val_sens: 0.9571 - val_dice_coef_loss: 0.0401\n",
      "\n",
      "Epoch 00039: val_dice_coef_loss did not improve from 0.03797\n",
      "Epoch 40/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0173 - accuracy: 0.9981 - sens: 0.9845 - dice_coef_loss: 0.0173 - val_loss: 0.2747 - val_accuracy: 0.9773 - val_sens: 0.5998 - val_dice_coef_loss: 0.2743\n",
      "\n",
      "Epoch 00040: val_dice_coef_loss did not improve from 0.03797\n",
      "Epoch 41/200\n",
      "8104/8104 [==============================] - 247s 30ms/step - loss: 0.0202 - accuracy: 0.9978 - sens: 0.9818 - dice_coef_loss: 0.0202 - val_loss: 0.0401 - val_accuracy: 0.9958 - val_sens: 0.9629 - val_dice_coef_loss: 0.0401\n",
      "\n",
      "Epoch 00041: val_dice_coef_loss did not improve from 0.03797\n",
      "Epoch 42/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0173 - accuracy: 0.9981 - sens: 0.9845 - dice_coef_loss: 0.0173 - val_loss: 0.0447 - val_accuracy: 0.9953 - val_sens: 0.9576 - val_dice_coef_loss: 0.0447\n",
      "\n",
      "Epoch 00042: val_dice_coef_loss did not improve from 0.03797\n",
      "Epoch 43/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0166 - accuracy: 0.9982 - sens: 0.9852 - dice_coef_loss: 0.0166 - val_loss: 0.0390 - val_accuracy: 0.9960 - val_sens: 0.9667 - val_dice_coef_loss: 0.0390\n",
      "\n",
      "Epoch 00043: val_dice_coef_loss did not improve from 0.03797\n",
      "Epoch 44/200\n",
      "8104/8104 [==============================] - 247s 30ms/step - loss: 0.0163 - accuracy: 0.9982 - sens: 0.9854 - dice_coef_loss: 0.0163 - val_loss: 0.0456 - val_accuracy: 0.9952 - val_sens: 0.9571 - val_dice_coef_loss: 0.0455\n",
      "\n",
      "Epoch 00044: val_dice_coef_loss did not improve from 0.03797\n",
      "Epoch 45/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0186 - accuracy: 0.9980 - sens: 0.9830 - dice_coef_loss: 0.0186 - val_loss: 0.0378 - val_accuracy: 0.9961 - val_sens: 0.9628 - val_dice_coef_loss: 0.0378\n",
      "\n",
      "Epoch 00045: val_dice_coef_loss improved from 0.03797 to 0.03780, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_45_0.018624.h5\n",
      "Epoch 46/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0165 - accuracy: 0.9982 - sens: 0.9850 - dice_coef_loss: 0.0165 - val_loss: 0.0381 - val_accuracy: 0.9960 - val_sens: 0.9638 - val_dice_coef_loss: 0.0381\n",
      "\n",
      "Epoch 00046: val_dice_coef_loss did not improve from 0.03780\n",
      "Epoch 47/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0174 - accuracy: 0.9981 - sens: 0.9842 - dice_coef_loss: 0.0174 - val_loss: 0.0392 - val_accuracy: 0.9959 - val_sens: 0.9642 - val_dice_coef_loss: 0.0391\n",
      "\n",
      "Epoch 00047: val_dice_coef_loss did not improve from 0.03780\n",
      "Epoch 48/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0157 - accuracy: 0.9983 - sens: 0.9858 - dice_coef_loss: 0.0157 - val_loss: 0.0375 - val_accuracy: 0.9961 - val_sens: 0.9630 - val_dice_coef_loss: 0.0374\n",
      "\n",
      "Epoch 00048: val_dice_coef_loss improved from 0.03780 to 0.03743, saving model to ./4_result/exp1/snapshot/LAT/lat_aug_exp1_48_0.015689.h5\n",
      "Epoch 49/200\n",
      "8104/8104 [==============================] - 247s 31ms/step - loss: 0.0154 - accuracy: 0.9983 - sens: 0.9860 - dice_coef_loss: 0.0154 - val_loss: 0.0396 - val_accuracy: 0.9959 - val_sens: 0.9613 - val_dice_coef_loss: 0.0396\n",
      "\n",
      "Epoch 00049: val_dice_coef_loss did not improve from 0.03743\n",
      "Epoch 50/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0154 - accuracy: 0.9984 - sens: 0.9860 - dice_coef_loss: 0.0153 - val_loss: 0.0397 - val_accuracy: 0.9958 - val_sens: 0.9590 - val_dice_coef_loss: 0.0397\n",
      "\n",
      "Epoch 00050: val_dice_coef_loss did not improve from 0.03743\n",
      "Epoch 51/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0154 - accuracy: 0.9983 - sens: 0.9859 - dice_coef_loss: 0.0155 - val_loss: 0.0405 - val_accuracy: 0.9958 - val_sens: 0.9664 - val_dice_coef_loss: 0.0404\n",
      "\n",
      "Epoch 00051: val_dice_coef_loss did not improve from 0.03743\n",
      "Epoch 52/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0166 - accuracy: 0.9982 - sens: 0.9849 - dice_coef_loss: 0.0166 - val_loss: 0.0390 - val_accuracy: 0.9960 - val_sens: 0.9619 - val_dice_coef_loss: 0.0390\n",
      "\n",
      "Epoch 00052: val_dice_coef_loss did not improve from 0.03743\n",
      "Epoch 53/200\n",
      "8104/8104 [==============================] - 248s 31ms/step - loss: 0.0171 - accuracy: 0.9982 - sens: 0.9842 - dice_coef_loss: 0.0171 - val_loss: 0.0383 - val_accuracy: 0.9960 - val_sens: 0.9633 - val_dice_coef_loss: 0.0382\n",
      "\n",
      "Epoch 00053: val_dice_coef_loss did not improve from 0.03743\n",
      "Epoch 54/200\n",
      "8104/8104 [==============================] - 249s 31ms/step - loss: 0.0146 - accuracy: 0.9984 - sens: 0.9867 - dice_coef_loss: 0.0146 - val_loss: 0.0387 - val_accuracy: 0.9960 - val_sens: 0.9609 - val_dice_coef_loss: 0.0387\n",
      "\n",
      "Epoch 00054: val_dice_coef_loss did not improve from 0.03743\n",
      "Epoch 55/200\n",
      "8104/8104 [==============================] - 245s 30ms/step - loss: 0.0141 - accuracy: 0.9985 - sens: 0.9871 - dice_coef_loss: 0.0141 - val_loss: 0.0377 - val_accuracy: 0.9961 - val_sens: 0.9648 - val_dice_coef_loss: 0.0377\n",
      "\n",
      "Epoch 00055: val_dice_coef_loss did not improve from 0.03743\n",
      "Epoch 56/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0141 - accuracy: 0.9985 - sens: 0.9871 - dice_coef_loss: 0.0141 - val_loss: 0.0411 - val_accuracy: 0.9957 - val_sens: 0.9597 - val_dice_coef_loss: 0.0411\n",
      "\n",
      "Epoch 00056: val_dice_coef_loss did not improve from 0.03743\n",
      "Epoch 57/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0152 - accuracy: 0.9984 - sens: 0.9861 - dice_coef_loss: 0.0152 - val_loss: 0.0402 - val_accuracy: 0.9958 - val_sens: 0.9640 - val_dice_coef_loss: 0.0402\n",
      "\n",
      "Epoch 00057: val_dice_coef_loss did not improve from 0.03743\n",
      "Epoch 58/200\n",
      "8104/8104 [==============================] - 246s 30ms/step - loss: 0.0137 - accuracy: 0.9985 - sens: 0.9875 - dice_coef_loss: 0.0137 - val_loss: 0.0382 - val_accuracy: 0.9960 - val_sens: 0.9663 - val_dice_coef_loss: 0.0382\n",
      "\n",
      "Epoch 00058: val_dice_coef_loss did not improve from 0.03743\n",
      "save model\n",
      "predict test data\n",
      "254/254 [==============================] - 12s 48ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Fitting model...')\n",
    "model.fit(imgs_train, imgs_mask_train, batch_size=16, epochs=200, verbose=1, validation_split=0.2, shuffle=True, callbacks=[model_checkpoint, earlystopping])\n",
    "\n",
    "print('save model')\n",
    "model.save(predict_path+'lat_aug_exp1.h5')\n",
    "\n",
    "print('predict test data')\n",
    "imgs_mask_test = model.predict(imgs_test, batch_size=1, verbose=1)\n",
    "np.save(predict_path+'prediction_lat_aug_exp1.npy', imgs_mask_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 512, 512, 1)\n",
      "complete\n",
      "acc avg : 0.9975\n",
      "sensitivity avg : 0.9559\n",
      "specificity avg : 0.9986\n",
      "dsc avg : 0.9484\n",
      "save file\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import r2_score\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "\n",
    "name_list=np.load('./3_deepdata/1_exp1/npy/LAT/test_name.npy')\n",
    "df = pd.DataFrame(columns=['name', 'acc', 'sen', 'spe', 'dsc'])\n",
    "\n",
    "true_list=np.load('./3_deepdata/1_exp1/npy/LAT/test_label.npy')\n",
    "true_list=true_list.astype('float32')\n",
    "true_list = true_list/255.0\n",
    "true_list[true_list > 0.5] = 1\n",
    "true_list[true_list <= 0.5] = 0\n",
    "print(true_list.shape)\n",
    "\n",
    "pred_list=np.load('./4_result/exp1/result/LAT/prediction_lat_aug_exp1.npy')\n",
    "pred_list[pred_list > 0.5] = 1\n",
    "pred_list[pred_list <= 0.5] = 0\n",
    "# pred_list[pred_list > 127] = 1\n",
    "# pred_list[pred_list <= 127] = 0\n",
    "\n",
    "sensitivity=[]\n",
    "specificity=[]\n",
    "acc=[]\n",
    "dsc=[]\n",
    "\n",
    "for i in range(len(true_list)):\n",
    "    yt=true_list[i].flatten()\n",
    "    yp=pred_list[i].flatten()\n",
    "    mat=confusion_matrix(yt,yp)\n",
    "    if len(mat) == 2:\n",
    "        ac=(mat[1,1]+mat[0,0])/(mat[1,0]+mat[1,1]+mat[0,1]+mat[0,0])\n",
    "        st=mat[1,1]/(mat[1,0]+mat[1,1])\n",
    "        sp=mat[0,0]/(mat[0,1]+mat[0,0])\n",
    "        if mat[1,0]+mat[1,1] == 0:\n",
    "            specificity.append(sp)\n",
    "            acc.append(ac)\n",
    "        else:\n",
    "            sensitivity.append(st)  \n",
    "            specificity.append(sp)\n",
    "            acc.append(ac)\n",
    "    else:\n",
    "        specificity.append(1)\n",
    "        acc.append(1)\n",
    "\n",
    "for i in range(len(true_list)):\n",
    "    yt=true_list[i]\n",
    "    yp=pred_list[i]\n",
    "    if np.sum(yt) != 0 and np.sum(yp) != 0:\n",
    "        dice = np.sum(yp[yt==1])*2.0 / (np.sum(yt) + np.sum(yp))\n",
    "        dsc.append(dice)\n",
    "    df=  df.append({'name':name_list[i], 'acc':ac, 'sen':st, 'spe':sp, 'dsc':dice}, ignore_index=True)\n",
    "\n",
    "print(\"complete\")     \n",
    "print(\"acc avg : {0:0.4f}\".format(np.mean(acc)))\n",
    "print(\"sensitivity avg : {0:0.4f}\".format(np.mean(sensitivity)))\n",
    "print(\"specificity avg : {0:0.4f}\".format(np.mean(specificity)))\n",
    "print(\"dsc avg : {0:0.4f}\".format(np.mean(dsc)))\n",
    "\n",
    "# print(\"sensitivity min:\",np.min(sensitivity))\n",
    "# print(\"specificity min:\",np.min(specificity))\n",
    "# print(\"dsc min:\",np.min(dsc))\n",
    "# print(\"acc min:\",np.min(acc))\n",
    "\n",
    "# print(\"sensitivity max:\",np.max(sensitivity))\n",
    "# print(\"specificity max:\",np.max(specificity))\n",
    "# print(\"dsc max:\",np.max(dsc))\n",
    "# print(\"acc max:\",np.max(acc))\n",
    "\n",
    "print('save file')\n",
    "df.to_csv('./4_result/exp1/result/LAT/lat_aug_exp1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array to image\n"
     ]
    }
   ],
   "source": [
    "pred_img_path = './4_result/exp1/result/LAT/exp1_img/'\n",
    "if not os.path.isdir(pred_img_path):\n",
    "    os.makedirs(pred_img_path)\n",
    "\n",
    "# pred_list=np.load('./4_ap_pre_pred/ap_pre_exp2.npy')\n",
    "# pred_list[pred_list > 0.5] = 1\n",
    "# pred_list[pred_list <= 0.5] = 0\n",
    "\n",
    "# name_list=np.load('./2_AP_pre_npy/test_pre_name.npy')\n",
    "print(\"array to image\")\n",
    "imgs = pred_list\n",
    "for i in range(imgs.shape[0]):\n",
    "    img = imgs[i]\n",
    "    img[img <= 0.5] = 0\n",
    "    img[img > 0.5] = 255\n",
    "    img = array_to_img(img)\n",
    "    img.save(pred_img_path+\"%s_pred.png\" %(name_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array to overlay image\n"
     ]
    }
   ],
   "source": [
    "pred_overlay_path = './4_result/exp1/result/LAT/exp1_overlay/'\n",
    "if not os.path.isdir(pred_overlay_path):\n",
    "    os.makedirs(pred_overlay_path)\n",
    "    \n",
    "print(\"array to overlay image\")\n",
    "imgs = pred_list\n",
    "gts = true_list\n",
    "ori_path = glob.glob('./3_deepdata/1_exp1/test/LAT_pre_img/' + '*.png')\n",
    "\n",
    "for i in range(imgs.shape[0]):\n",
    "    img = imgs[i]\n",
    "    ori_imgs = load_img(ori_path[i])\n",
    "    ori_imgs = ori_imgs.resize((512,512))\n",
    "    ori_img = img_to_array(ori_imgs)\n",
    "    img_name = ori_path[i][ori_path[i].rindex('/')+1:ori_path[i].rindex('.')]\n",
    "#     print(img_name)\n",
    "    img[img <= 0.5] = 0\n",
    "    img[img > 0.5] = 255\n",
    "    img = img_to_array(img)\n",
    "    \n",
    "    ori_img[:,:,0] = img[:,:,0]+ori_img[:,:,0]\n",
    "    ori_img[:,:,0][ori_img[:,:,0]>255]=255\n",
    "    ori_img = array_to_img(ori_img)\n",
    "    ori_img.save(pred_overlay_path+\"{}_pred.png\".format(img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array to overlay image\n"
     ]
    }
   ],
   "source": [
    "pred_overlay_path = './4_result/exp1/result/LAT/exp1_overlay_tpfnfp/'\n",
    "if not os.path.isdir(pred_overlay_path):\n",
    "    os.makedirs(pred_overlay_path)\n",
    "    \n",
    "print(\"array to overlay image\")\n",
    "imgs = pred_list\n",
    "gts = true_list\n",
    "ori_path = glob.glob('./3_deepdata/1_exp1/test/LAT_pre_img/' + '*.png')\n",
    "\n",
    "for i in range(imgs.shape[0]):\n",
    "    img = imgs[i]\n",
    "    gt = gts[i]\n",
    "    ori_imgs = load_img(ori_path[i])\n",
    "    ori_imgs = ori_imgs.resize((512,512))\n",
    "    ori_img = img_to_array(ori_imgs)\n",
    "    img_name = ori_path[i][ori_path[i].rindex('/')+1:ori_path[i].rindex('.')]\n",
    "#     print(img_name)\n",
    "    img[img <= 0.5] = 0\n",
    "    img[img > 0.5] = 255\n",
    "    img = img_to_array(img)\n",
    "    gt[gt <= 0.5] = 0\n",
    "    gt[gt > 0.5] = 255\n",
    "    gt = img_to_array(gt)\n",
    "    \n",
    "    ori_img[:,:,0] = img[:,:,0]+ori_img[:,:,0]\n",
    "    ori_img[:,:,0][ori_img[:,:,0]>255]=255\n",
    "    \n",
    "    ori_img[:,:,1] = gt[:,:,0]+ori_img[:,:,1]\n",
    "    ori_img[:,:,1][ori_img[:,:,1]>255]=255\n",
    "#     print(np.unique(ori_img[:,:,2]))\n",
    "    pred_img = array_to_img(ori_img)\n",
    "    pred_img.save(pred_overlay_path+\"{}_pred.png\".format(img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "def max_contour(img):\n",
    "    contours, _ = cv2.findContours(img, cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "    tmp =[]\n",
    "#     print(len(contours))\n",
    "    for k in range(len(contours)):\n",
    "        cnt = contours[k]\n",
    "        mmt = cv2.moments(cnt) \n",
    "        tmp.append(float(mmt['m00']))\n",
    "\n",
    "    max_num = max(tmp)\n",
    "    index = tmp.index(max_num)\n",
    "#     contour = contours[index]\n",
    "    return contours, index\n",
    "    \n",
    "def postprocessing(img):\n",
    "    img = img.astype('uint8')\n",
    "#     print(img.dtype)\n",
    "    ret, thresh = cv2.threshold(img,127,255,0)\n",
    "    img = ndimage.binary_fill_holes(thresh).astype('uint8')\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "#     print(img.shape)\n",
    "    \n",
    "    contours, max_cnt_index = max_contour(img)\n",
    "    \n",
    "    for i, contour in enumerate(contours):\n",
    "        if i!=max_cnt_index:\n",
    "            cv2.fillPoly(img, [contour], color=(0,0,0))\n",
    "#         else:\n",
    "#             cv2.fillPoly(img, [contour], color=(255,255,255))\n",
    "    kernel = np.ones((7,7), np.uint8)\n",
    "    morph_cnt = 8\n",
    "#     imshow_plt(img)\n",
    "#     print(img.shape)\n",
    "    img_m = img.copy()\n",
    "    for j in range(morph_cnt):\n",
    "        img_m = cv2.morphologyEx(img_m, cv2.MORPH_DILATE, kernel)\n",
    "#     print(img.shape)\n",
    "#     imshow_plt(img)\n",
    "    for k in range(morph_cnt):\n",
    "        img_m = cv2.morphologyEx(img_m, cv2.MORPH_ERODE, kernel)\n",
    "#     imshow_plt(img)\n",
    "    return img_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_path = sorted(glob.glob('./4_result/exp1/result/LAT/exp1_img/*.png'))\n",
    "\n",
    "post_img_path = './4_result/exp1/result/LAT/exp1_postprocessing/'\n",
    "if not os.path.isdir(post_img_path):\n",
    "    os.makedirs(post_img_path)\n",
    "    \n",
    "for i, img_path in enumerate(imgs_path):\n",
    "    img_name = img_path[img_path.rindex('/')+1:img_path.rindex('_')]\n",
    "#     print(img_name)\n",
    "\n",
    "#     if i==10:\n",
    "#         break\n",
    "    img = cv2.imread(img_path,0)\n",
    "    post_img = postprocessing(img)\n",
    "    post_img = cv2.cvtColor(post_img, cv2.COLOR_GRAY2BGR)\n",
    "#     print(post_img.shape)\n",
    "#     plt.figure(figsize=(15,15))\n",
    "#     plt.imshow(post_img[:,:,0], cmap='gray')\n",
    "#     plt.show()\n",
    "    \n",
    "    post_img = array_to_img(post_img)\n",
    "    post_img.save(post_img_path+\"{}_post.png\".format(img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array to overlay image\n"
     ]
    }
   ],
   "source": [
    "pred_overlay_path = './4_result/exp1/result/LAT/exp1_overlay_post/'\n",
    "if not os.path.isdir(pred_overlay_path):\n",
    "    os.makedirs(pred_overlay_path)\n",
    "    \n",
    "print(\"array to overlay image\")\n",
    "imgs = pred_list\n",
    "ori_path = glob.glob('./3_deepdata/1_exp1/test/LAT_pre_img/' + '*.png')\n",
    "\n",
    "for i in range(imgs.shape[0]):\n",
    "    img = imgs[i]\n",
    "    ori_imgs = load_img(ori_path[i])\n",
    "    ori_imgs = ori_imgs.resize((512,512))\n",
    "    ori_img = img_to_array(ori_imgs)\n",
    "    img_name = ori_path[i][ori_path[i].rindex('/')+1:ori_path[i].rindex('.')]\n",
    "#     print(img_name)\n",
    "    img[img <= 0.5] = 0\n",
    "    img[img > 0.5] = 255\n",
    "    img = img_to_array(img)\n",
    "    post_img = postprocessing(img)\n",
    "    post_img = cv2.cvtColor(post_img, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    ori_img[:,:,0] = post_img[:,:,0]*255+ori_img[:,:,0]\n",
    "    ori_img[:,:,0][ori_img[:,:,0]>255]=255\n",
    "    \n",
    "    pred_img = array_to_img(ori_img)\n",
    "    pred_img.save(pred_overlay_path+\"{}_pred.png\".format(img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array to overlay image\n"
     ]
    }
   ],
   "source": [
    "pred_overlay_path = './4_result/exp1/result/LAT/exp1_overlay_post_tpfnfp/'\n",
    "if not os.path.isdir(pred_overlay_path):\n",
    "    os.makedirs(pred_overlay_path)\n",
    "    \n",
    "print(\"array to overlay image\")\n",
    "imgs = pred_list\n",
    "ori_path = glob.glob('./3_deepdata/1_exp1/test/LAT_pre_img/' + '*.png')\n",
    "\n",
    "for i in range(imgs.shape[0]):\n",
    "#     if i==1:\n",
    "#         break\n",
    "    img = imgs[i]\n",
    "    ori_imgs = load_img(ori_path[i])\n",
    "    ori_imgs = ori_imgs.resize((512,512))\n",
    "    ori_img = img_to_array(ori_imgs)\n",
    "    img_name = ori_path[i][ori_path[i].rindex('/')+1:ori_path[i].rindex('.')]\n",
    "#     print(img_name)\n",
    "    img[img <= 0.5] = 0\n",
    "    img[img > 0.5] = 255\n",
    "    img = img_to_array(img)\n",
    "    post_img = postprocessing(img)\n",
    "    post_img = cv2.cvtColor(post_img, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    ori_img[:,:,0] = post_img[:,:,0]*255+ori_img[:,:,0]\n",
    "    ori_img[:,:,0][ori_img[:,:,0]>255]=255\n",
    "    \n",
    "#     ori_img[:,:,1] = img[:,:,0]+ori_img[:,:,1]\n",
    "#     ori_img[:,:,1][ori_img[:,:,1]>255]=255\n",
    "    \n",
    "    \n",
    "    ori_img[:,:,1] = img[:,:,0]+ori_img[:,:,1]\n",
    "    ori_img[:,:,1][ori_img[:,:,1]>255]=255\n",
    "    \n",
    "    pred_img = array_to_img(ori_img)\n",
    "    pred_img.save(pred_overlay_path+\"{}_pred.png\".format(img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_plt(img):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    if len(img.shape)==2:\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "    plt.imshow(img[:,:,0], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254, 512, 512, 1)\n",
      "complete\n",
      "acc avg : 0.9970\n",
      "sensitivity avg : 0.9660\n",
      "specificity avg : 0.9978\n",
      "dsc avg : 0.9393\n",
      "save file\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import r2_score\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "\n",
    "name_list=np.load('./3_deepdata/1_exp1/npy/LAT/test_name.npy')\n",
    "df = pd.DataFrame(columns=['name', 'acc', 'sen', 'spe', 'dsc'])\n",
    "\n",
    "true_list=np.load('./3_deepdata/1_exp1/npy/LAT/test_label.npy')\n",
    "true_list=true_list.astype('float32')\n",
    "true_list = true_list/255.0\n",
    "true_list[true_list > 0.5] = 1\n",
    "true_list[true_list <= 0.5] = 0\n",
    "print(true_list.shape)\n",
    "\n",
    "pred_list=np.array([cv2.imread(path, 0) for path in sorted(glob.glob('./4_result/exp1/result/LAT/exp1_postprocessing/*.png'))])\n",
    "pred_list = np.expand_dims(pred_list, axis=-1)\n",
    "pred_list = pred_list/255.0\n",
    "# pred_list[pred_list > 0.5] = 1\n",
    "# pred_list[pred_list <= 0.5] = 0\n",
    "# pred_list[pred_list > 127] = 1\n",
    "# pred_list[pred_list <= 127] = 0\n",
    "\n",
    "sensitivity=[]\n",
    "specificity=[]\n",
    "acc=[]\n",
    "dsc=[]\n",
    "\n",
    "for i in range(len(true_list)):\n",
    "    yt=true_list[i].flatten()\n",
    "    yp=pred_list[i].flatten()\n",
    "    mat=confusion_matrix(yt,yp)\n",
    "    if len(mat) == 2:\n",
    "        ac=(mat[1,1]+mat[0,0])/(mat[1,0]+mat[1,1]+mat[0,1]+mat[0,0])\n",
    "        st=mat[1,1]/(mat[1,0]+mat[1,1])\n",
    "        sp=mat[0,0]/(mat[0,1]+mat[0,0])\n",
    "        if mat[1,0]+mat[1,1] == 0:\n",
    "            specificity.append(sp)\n",
    "            acc.append(ac)\n",
    "        else:\n",
    "            sensitivity.append(st)  \n",
    "            specificity.append(sp)\n",
    "            acc.append(ac)\n",
    "    else:\n",
    "        specificity.append(1)\n",
    "        acc.append(1)\n",
    "\n",
    "# for i in range(len(true_list)):\n",
    "    yt=true_list[i]\n",
    "    yp=pred_list[i]\n",
    "    if np.sum(yt) != 0 and np.sum(yp) != 0:\n",
    "        dice = np.sum(yp[yt==1])*2.0 / (np.sum(yt) + np.sum(yp))\n",
    "        dsc.append(dice)\n",
    "    df=  df.append({'name':name_list[i], 'acc':ac, 'sen':st, 'spe':sp, 'dsc':dice}, ignore_index=True)\n",
    "\n",
    "print(\"complete\")      \n",
    "print(\"acc avg : {0:0.4f}\".format(np.mean(acc)))\n",
    "print(\"sensitivity avg : {0:0.4f}\".format(np.mean(sensitivity)))\n",
    "print(\"specificity avg : {0:0.4f}\".format(np.mean(specificity)))\n",
    "print(\"dsc avg : {0:0.4f}\".format(np.mean(dsc)))\n",
    "\n",
    "# print(\"sensitivity min:\",np.min(sensitivity))\n",
    "# print(\"specificity min:\",np.min(specificity))\n",
    "# print(\"dsc min:\",np.min(dsc))\n",
    "# print(\"acc min:\",np.min(acc))\n",
    "\n",
    "# print(\"sensitivity max:\",np.max(sensitivity))\n",
    "# print(\"specificity max:\",np.max(specificity))\n",
    "# print(\"dsc max:\",np.max(dsc))\n",
    "# print(\"acc max:\",np.max(acc))\n",
    "\n",
    "print('save file')\n",
    "df.to_csv('./4_result/exp1/result/LAT/ap_aug_exp1_post.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
